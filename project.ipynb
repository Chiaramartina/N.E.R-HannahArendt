{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32b14084",
   "metadata": {},
   "source": [
    "**SpaCy** is a Python library for Natural Language Processing (NLP). It uses statistical models based on neural networks. These models are already trained on large corpora (texts) for:\n",
    "- **Tokenize**: it divides the text into tokens respecting language rules and dictionary.\n",
    "- Find speech parts (**POS tagging**): Assign a grammatical label to each word using statistical models.\n",
    "- Analyze grammatical dependencies (**Parsing**): Create a grammatical dependency tree (who depends on whom).\n",
    "- Recognise entities (**NER**): It detects sequences of tokens that correspond to entities (e.g. people, places).\n",
    "Internally it uses models like Convolutional Neural Networks (CNN).\n",
    "\n",
    "Documentation: https://spacy.io/usage/projects/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930cd096",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc --version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a1bc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade spacy\n",
    "!pip install --upgrade spacy[cuda111,transformers]\n",
    "!pip install jsonlines\n",
    "!python -m spacy download en_core_web_trf\n",
    "!pip install spacy-transformers\n",
    "!python -m spacy download en_core_web_trf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230991a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.autonotebook import tqdm\n",
    "import re\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c029d8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spaCy's Transformer-based language model (en_core_web_trf)\n",
    "nlp = spacy.load(\"en_core_web_trf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2a0ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"txt/portrait_of_a_Period.txt\", \"r\") as f:\n",
    "    articles = f.read()\n",
    "\n",
    "print(len(articles))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d7eabf",
   "metadata": {},
   "source": [
    "This function extracts all PERSON entities from the document and corrects any names that end in possessive form (e.g., 's) by removing the final part. This ensures that names like 'Stefan Zweig' and 'Stefan Zweigâ€™s' are treated as the same entity. Additionally, the function filters results to include only those names that begin with an uppercase letter, reducing noise from incorrect or generic matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bbb03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to filter PERSON entities (labeled by spaCy), removing:\n",
    "# entities that do not start with uppercase letters, entities with special characters or numbers,possessives (e.g., 'John's' is converted to 'John' using Span).\n",
    "# Returns a list of cleaned entities, ready for further analysis.\n",
    "def filter_person(doc):\n",
    "    filtered_spans = []\n",
    "    invalid_chars = re.compile(r\"[^a-zA-Z\\s]\")\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ != \"PERSON\":\n",
    "            continue\n",
    "\n",
    "        ent_text = ent.text.strip()\n",
    "\n",
    "        first_alpha = next((c for c in ent_text if c.isalpha()), None)\n",
    "        if not first_alpha or not first_alpha.isupper():\n",
    "            continue\n",
    "\n",
    "        if invalid_chars.search(ent_text):\n",
    "            continue\n",
    "\n",
    "        if ent_text.endswith(\"'s\"):\n",
    "            span = Span(doc, ent.start, ent.end - 1, label=ent.label_)\n",
    "            filtered_spans.append(span)\n",
    "        else:\n",
    "            filtered_spans.append(ent)\n",
    "\n",
    "    return filtered_spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748d2b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bbfd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_names = filter_person(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3e68b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.ents = filtered_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4e10b7",
   "metadata": {},
   "source": [
    "Graph display where each word is linked to another according to the grammatical structure (e.g. subject, object, main verb), with arrows indicating the directions of the dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb6ba04",
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(doc, style=\"dep\", jupyter=True, options={'distance': 140})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52e229d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the recognized entities (PERSON) by highlighting them in the text.\n",
    "displacy.render(doc, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bdd324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all unique person names and sort them alphabetically.\n",
    "persons = sorted(set(ent.text for ent in doc.ents if ent.label_ == \"PERSON\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c76cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules for XML creation and formatting, used to export entities in a structured format\n",
    "from xml.etree.ElementTree import Element, SubElement, tostring\n",
    "from xml.dom import minidom\n",
    "from xml.dom.minidom import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db779101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace names in the text with XML tags <name type=\"person\">...<name>\n",
    "def annotate_text(text, names):\n",
    "    annotated = text\n",
    "    placeholder_map = {}\n",
    "\n",
    "    # First step: replace full names with unique placeholders to avoid conflicts during substitution.\n",
    "    for i, name in enumerate(sorted(names, key=len, reverse=True)):\n",
    "        pattern = re.escape(name)\n",
    "        placeholder = f\"__PERSON_{i}__\"\n",
    "        placeholder_map[placeholder] = f'<name type=\"person\">{name}</name>'\n",
    "        annotated = re.sub(rf'(?<!\\w){pattern}(?!\\w)', placeholder, annotated)\n",
    "\n",
    "    # Second step: replace placeholders with XML tags.\n",
    "    for placeholder, tag in placeholder_map.items():\n",
    "        annotated = annotated.replace(placeholder, tag)\n",
    "\n",
    "    return annotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd36708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the annotation function to the original text to generate the marked XML version\n",
    "annotated_text = annotate_text(articles, persons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f790fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(annotated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6ad865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the annotated text in .xml format\n",
    "with open(\"annotated_txt.xml\", \"w\", encoding=\"utf-8\") as out_file:\n",
    "    out_file.write(annotated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079da35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create unique IDs (based on the first 3 letters of the name).\n",
    "used_ids = set()\n",
    "\n",
    "def unique_id(name):\n",
    "    base = name[0].upper() + name[1].upper() + name[2].upper()\n",
    "    candidate = base\n",
    "    used_ids.add(candidate)\n",
    "    return candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d62de07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove partial names already included in full names (e.g., 'Zweig' if 'Stefan Zweig' is also present) to avoid redundancy\n",
    "def filter_partial_names(person_list):\n",
    "    full_names = set()\n",
    "    partials_to_remove = set()\n",
    "\n",
    "    normalized = [p.strip() for p in person_list]\n",
    "\n",
    "    for name in normalized:\n",
    "        for other in normalized:\n",
    "            if name != other and name in other.split() and len(other.split()) > 1:\n",
    "                partials_to_remove.add(name)\n",
    "                break\n",
    "\n",
    "    return [name for name in normalized if name not in partials_to_remove]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853d36ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TEI XML structure (standard for text encoding) and prepare a listPerson section.\n",
    "doc_xml = Document()\n",
    "list_elem = doc_xml.createElement(\"list\")\n",
    "persons = filter_partial_names(persons)\n",
    "\n",
    "for person in sorted(persons):\n",
    "\n",
    "    item = doc_xml.createElement(\"item\")\n",
    "    xml_id = unique_id(person)\n",
    "    item.setAttribute(\"xml:id\", xml_id)\n",
    "\n",
    "    name_elem = doc_xml.createElement(\"name\")\n",
    "    name_elem.setAttribute(\"type\", \"person\")\n",
    "    name_text = doc_xml.createTextNode(person)\n",
    "    name_elem.appendChild(name_text)\n",
    "\n",
    "    item.appendChild(name_elem)\n",
    "    list_elem.appendChild(item)\n",
    "\n",
    "doc_xml.appendChild(list_elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df043869",
   "metadata": {},
   "outputs": [],
   "source": [
    "tei = Element('TEI')\n",
    "teiHeader = SubElement(tei, 'teiHeader')\n",
    "text_elem = SubElement(tei, 'text')\n",
    "back = SubElement(text_elem, 'back')\n",
    "listPerson = SubElement(back, 'listPerson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8d8fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doc_xml.toprettyxml(indent=\"  \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c041287",
   "metadata": {},
   "source": [
    "### LIMIT:\n",
    "\n",
    "SpaCy labels concepts such as \"Dasein\", \"Existenz\", \"Selves\" as PERSON entities because they start with an uppercase letter. This can happen for several reasons, related to how NER models work in spaCy. Main reasons:\n",
    "- Capitalization rules: SpaCy, like many NER models, has been trained on a large corpus of texts, including news articles and general texts, where proper nouns of people, cities, companies, etc., are often capitalized. When the model encounters a word starting with an uppercase letter, it might erroneously classify it as a person, especially if that word corresponds to a name seen in the training data as PERSON. Concepts like Dasein or Man are capitalized in many philosophical texts, which might cause spaCy to label them as people.\n",
    "- Semantic ambiguity in philosophical contexts: In philosophical texts, capitalization is often tied to concepts, such as in the case of Dasein (a term introduced by Heidegger) or Existenz (related to existentialism). SpaCy, being a general-purpose model, cannot make this distinction between a proper noun for a person and philosophical concepts. A model specifically trained on philosophical writings would be required to handle this distinction."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
