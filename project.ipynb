{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32b14084",
   "metadata": {},
   "source": [
    "**SpaCy** is a Python library for Natural Language Processing (NLP). It uses statistical models based on neural networks. These models are already trained on large corpora (texts) for:\n",
    "- **Tokenize**: it divides the text into tokens respecting language rules and dictionary.\n",
    "- Find speech parts (**POS tagging**): Assign a grammatical label to each word using statistical models.\n",
    "- Analyze grammatical dependencies (**Parsing**): Create a grammatical dependency tree (who depends on whom).\n",
    "- Recognise entities (**NER**): It detects sequences of tokens that correspond to entities (e.g. people, places).\n",
    "Internally it uses models like Convolutional Neural Networks (CNN).\n",
    "\n",
    "Documentation: https://spacy.io/usage/projects/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930cd096",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc --version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a1bc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade spacy\n",
    "!pip install --upgrade spacy[cuda111,transformers]\n",
    "!pip install jsonlines\n",
    "!python -m spacy download en_core_web_lg\n",
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download en_core_web_trf\n",
    "!pip install spacy-transformers\n",
    "!python -m spacy download en_core_web_trf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230991a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.autonotebook import tqdm\n",
    "import re\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c029d8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_trf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2a0ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"txt/portrait_of_a_Period.txt\", \"r\") as f:\n",
    "    articles = f.read()\n",
    "\n",
    "print(len(articles))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d7eabf",
   "metadata": {},
   "source": [
    "This function extracts all PERSON entities from the document and corrects any names that end in possessive form (e.g., 's) by removing the final part. This ensures that names like 'Stefan Zweig' and 'Stefan Zweig’s' are treated as the same entity. Additionally, the function filters results to include only those names that begin with an uppercase letter, reducing noise from incorrect or generic matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bbb03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_person(doc):\n",
    "    filtered_spans = []\n",
    "    invalid_chars = re.compile(r\"[^a-zA-Z\\s]\")\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ != \"PERSON\":\n",
    "            continue\n",
    "\n",
    "        ent_text = ent.text.strip()\n",
    "\n",
    "        first_alpha = next((c for c in ent_text if c.isalpha()), None)\n",
    "        if not first_alpha or not first_alpha.isupper():\n",
    "            continue\n",
    "\n",
    "        if invalid_chars.search(ent_text):\n",
    "            continue\n",
    "\n",
    "        if ent_text.endswith(\"'s\"):\n",
    "            span = Span(doc, ent.start, ent.end - 1, label=ent.label_)\n",
    "            filtered_spans.append(span)\n",
    "        else:\n",
    "            filtered_spans.append(ent)\n",
    "\n",
    "    return filtered_spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eaf28d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_full_and_partial_names(name_dict):\n",
    "    matches = []\n",
    "    items = list(name_dict.items())\n",
    "\n",
    "    for i, (id1, name1) in enumerate(items):\n",
    "        name1_parts = name1.split()\n",
    "\n",
    "        for j, (id2, name2) in enumerate(items):\n",
    "            if id1 == id2:\n",
    "                continue\n",
    "\n",
    "            # Se name2 è una delle parti di name1 (es. \"Zweig\" in \"Stefan Zweig\")\n",
    "            if name2 in name1_parts:\n",
    "                matches.append((id1, id2))  # id1 ha il nome completo, id2 solo il cognome\n",
    "\n",
    "    return matches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748d2b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bbfd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_names = filter_person(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3e68b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.ents = filtered_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4e10b7",
   "metadata": {},
   "source": [
    "Graph display where each word is linked to another according to the grammatical structure (e.g. subject, object, main verb), with arrows indicating the directions of the dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb6ba04",
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(doc, style=\"dep\", jupyter=True, options={'distance': 140})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52e229d",
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(doc, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bdd324",
   "metadata": {},
   "outputs": [],
   "source": [
    "persons = sorted(set(ent.text for ent in doc.ents if ent.label_ == \"PERSON\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c76cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xml.etree.ElementTree import Element, SubElement, tostring\n",
    "from xml.dom import minidom\n",
    "from xml.dom.minidom import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f29f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_id(name):\n",
    "    parts = name.strip().split()\n",
    "    if len(parts) >= 2:\n",
    "        return (parts[0][0] + parts[1][0]).upper()\n",
    "    else:\n",
    "        return parts[0][:3].upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063ec3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_ids = {}\n",
    "used_ids = set()\n",
    "\n",
    "for person in persons:\n",
    "    base_id = generate_id(person)\n",
    "    if base_id not in used_ids:\n",
    "        person_ids[person] = base_id\n",
    "        used_ids.add(base_id)\n",
    "    else:\n",
    "        person_ids[person] = base_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db779101",
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_text(text, names):\n",
    "    annotated = text\n",
    "    placeholder_map = {}\n",
    "\n",
    "    # Primo passaggio: sostituisci nomi completi con placeholder univoci\n",
    "    for i, name in enumerate(sorted(names, key=len, reverse=True)):\n",
    "        pattern = re.escape(name)\n",
    "        placeholder = f\"__PERSON_{i}__\"\n",
    "        placeholder_map[placeholder] = f'<name type=\"person\">{name}</name>'\n",
    "        annotated = re.sub(rf'(?<!\\w){pattern}(?!\\w)', placeholder, annotated)\n",
    "\n",
    "    # Secondo passaggio: sostituisci placeholder con tag XML\n",
    "    for placeholder, tag in placeholder_map.items():\n",
    "        annotated = annotated.replace(placeholder, tag)\n",
    "\n",
    "    return annotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd36708",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_text = annotate_text(articles, person_ids.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f790fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(annotated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6ad865",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"annotated_txt.xml\", \"w\", encoding=\"utf-8\") as out_file:\n",
    "    out_file.write(annotated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079da35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "used_ids = set()\n",
    "\n",
    "def unique_id(name):\n",
    "    base = name[0].upper() + name[1].upper() + name[2].upper()\n",
    "    candidate = base\n",
    "    used_ids.add(candidate)\n",
    "    return candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d62de07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtro nome e cognome\n",
    "def filter_partial_names(person_list):\n",
    "    full_names = set()\n",
    "    partials_to_remove = set()\n",
    "\n",
    "    normalized = [p.strip() for p in person_list]\n",
    "\n",
    "    for name in normalized:\n",
    "        for other in normalized:\n",
    "            if name != other and name in other.split() and len(other.split()) > 1:\n",
    "                partials_to_remove.add(name)\n",
    "                break\n",
    "\n",
    "    return [name for name in normalized if name not in partials_to_remove]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853d36ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_xml = Document()\n",
    "list_elem = doc_xml.createElement(\"list\")\n",
    "persons = filter_partial_names(persons)\n",
    "\n",
    "for person in sorted(persons):\n",
    "\n",
    "    item = doc_xml.createElement(\"item\")\n",
    "    xml_id = unique_id(person)\n",
    "    item.setAttribute(\"xml:id\", xml_id)\n",
    "\n",
    "    name_elem = doc_xml.createElement(\"name\")\n",
    "    name_elem.setAttribute(\"type\", \"person\")\n",
    "    name_text = doc_xml.createTextNode(person)\n",
    "    name_elem.appendChild(name_text)\n",
    "\n",
    "    item.appendChild(name_elem)\n",
    "    list_elem.appendChild(item)\n",
    "\n",
    "doc_xml.appendChild(list_elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67704c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_xml = Document()\n",
    "list_elem = doc_xml.createElement(\"list\")\n",
    "\n",
    "for person in sorted(persons):\n",
    "    item = doc_xml.createElement(\"item\")\n",
    "    xml_id = unique_id(person)\n",
    "    item.setAttribute(\"xml:id\", xml_id)\n",
    "\n",
    "    name_elem = doc_xml.createElement(\"name\")\n",
    "    name_elem.setAttribute(\"type\", \"person\")\n",
    "    name_text = doc_xml.createTextNode(person)\n",
    "    name_elem.appendChild(name_text)\n",
    "\n",
    "    item.appendChild(name_elem)\n",
    "    list_elem.appendChild(item)\n",
    "\n",
    "doc_xml.appendChild(list_elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df043869",
   "metadata": {},
   "outputs": [],
   "source": [
    "tei = Element('TEI')\n",
    "teiHeader = SubElement(tei, 'teiHeader')\n",
    "text_elem = SubElement(tei, 'text')\n",
    "back = SubElement(text_elem, 'back')\n",
    "listPerson = SubElement(back, 'listPerson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8d8fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doc_xml.toprettyxml(indent=\"  \"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
